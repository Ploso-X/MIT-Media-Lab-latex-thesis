{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\gensim-2.0.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our data\n",
    "Warning: you will receive an error message when trying to use nltk's stopwords if you don't explicitly download the stopwords first: \n",
    "```python\n",
    "import nltk\n",
    "nltk.download(\"stopwords\") \n",
    "```\n",
    "\n",
    "Loading the provided reviews subset JSON into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#df = pd.read_csv('reviews.csv')\n",
    "\n",
    "df = pd.read_table('reviews_2.csv',sep=',',header=None)\n",
    "df.columns = ['ID','numDate','prod','overall','helpful','votes','date','asin','summary','reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a nice corpus of text, lets go through some of the standard preprocessing required for almost any topic modeling or NLP problem.\n",
    "\n",
    "Our Approach will involve:\n",
    "1. Tokenizing: converting a document to its atomic elements\n",
    "2. Stopping: removing meaningless words\n",
    "3. Stemming: merging words that are equivalent in meaning\n",
    "\n",
    "### Tokenization\n",
    "We have many ways to segment our document into its atomic elements. To start we'll tokenize the document into words. For this instance we'll use NLTKâ€™s `tokenize.regexp` module. You can see how this works in a fun interactive way here: try 'w+' at http://regexr.com/:\n",
    "![alt text](http://kldavenport.com/wp-content/uploads/2017/03/regex.gif \"regexr.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running through part of the first review to demonstrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_1 = df.reviewText[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 characters in string vs 18 words in a list\n",
      "['whobbly', 'to', 'ride', 'going', 'around', 'corners', 'very', 'carefully', 'to', 'not']\n"
     ]
    }
   ],
   "source": [
    "# Using one of our docs as an example\n",
    "tokens = tokenizer.tokenize(doc_1.lower())\n",
    "\n",
    "print('{} characters in string vs {} words in a list'.format(len(doc_1),                                                             len(tokens)))\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "Determiners like \"the\" and conjunctions such as \"or\" and \"for\" do not add value to the simple topic model. These types of words are known as stop words and are desired to be removed them from our list of tokens. The definition of a stop work changes depending on the context of the examined documents. \n",
    "\n",
    "Super list of stop words from the `stop_words` and `nltk` package below.\n",
    "```python\n",
    "merged_stopwords = [*nltk_stpwd, *stop_words_stpwd]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "['i', \"hadn't\", \"you'd\", 'theirs', 'would', 'between', 't', 'nor', 'how', 'more']\n"
     ]
    }
   ],
   "source": [
    "nltk_stpwd = stopwords.words('english')\n",
    "stop_words_stpwd = get_stop_words('en')\n",
    "merged_stopwords = list(set(nltk_stpwd + stop_words_stpwd))\n",
    "\n",
    "print(len(set(merged_stopwords)))\n",
    "print(merged_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whobbly', 'ride', 'going', 'around', 'corners', 'carefully', 'tip', 'product', 'arrived', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "stopped_tokens = [token for token in tokens if not token in merged_stopwords]\n",
    "print(stopped_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming allows the reduction of inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance, running and runner to run. Another example:\n",
    "\n",
    "*Amazon's catalog contains bike tires in different sizes and colors $\\Rightarrow$ Amazon catalog contain bike tire in differ size and color*\n",
    "\n",
    "Stemming is a basic and crude heuristic compared to [Lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) which understands vocabulary and morphological analysis instead of lobbing off the end of words. Essentially Lemmatization removes inflectional endings to return the word to its base or dictionary form of a word, which is defined as the lemma. Great illustrative examples from Wikipedia:\n",
    "1. *The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.*\n",
    "2. *The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.*\n",
    "3. *The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context.*\n",
    "\n",
    "I start with the common [Snowball stemming method](http://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg), a successor of sorts of the original Porter Stemmer which is implemented in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate a Snowball stemmer\n",
    "sb_stemmer = SnowballStemmer('english')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that p_stemmer requires all tokens to be type str. p_stemmer returns the string parameter in stemmed form, so we need to loop through our stopped_tokens:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whobbl', 'ride', 'go', 'around', 'corner', 'care', 'tip', 'product', 'arriv', 'excel', 'condit']\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together a document-term matrix\n",
    "\n",
    "In order to create an LDA model the 3 steps from above (tokenizing, stopping, stemming) are needed together to create a list of documents (list of lists) to then generate a document-term matrix (unique terms as rows, documents or reviews as columns). This matrix will tell how frequently each term occurs with each individual document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 439 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_reviews = df.shape[0]\n",
    "\n",
    "doc_set = [df.reviewText[i] for i in range(num_reviews)]\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in doc_set:\n",
    "    # putting our three steps together\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    stopped_tokens = [token for token in tokens if not token in merged_stopwords]\n",
    "    stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['whobbl', 'ride', 'go', 'around', 'corner', 'care', 'tip', 'product', 'arriv', 'excel', 'condit']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0]) # examine review 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform tokenized documents into an id-term dictionary\n",
    "Gensim's Dictionary method encapsulates the mapping between normalized words and their integer ids. Note a term will have an id of some number and in the subsequent bag of words step we can see that id will have a count associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(2343 unique tokens: ['faster', 'thick', 'guard', 'tour', 'teas']...)\n"
     ]
    }
   ],
   "source": [
    "# Gensim's Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
    "texts_dict = corpora.Dictionary(texts)\n",
    "texts_dict.save('auto_review.dict') # lets save to disk for later use\n",
    "# Examine each tokenâ€™s unique id\n",
    "print(texts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the mapping between words and their ids we can use the `token2id` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs 1 through 10: [('tip', 0), ('corner', 1), ('product', 2), ('go', 3), ('ride', 4), ('condit', 5), ('whobbl', 6), ('around', 7), ('arriv', 8), ('care', 9)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "print(\"IDs 1 through 10: {}\".format(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    634\n",
      "True      36\n",
      "Name: reviewText, dtype: int64\n",
      "\n",
      "False    625\n",
      "True      45\n",
      "Name: reviewText, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Guess the original work and examine the count difference between the #1 most frequent term and the #10 most frequent term:\n",
    "\n",
    "print(df.reviewText.str.contains(\"balance\").value_counts())\n",
    "print()\n",
    "print(df.reviewText.str.contains(\"lot\").value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a lot of unique tokens, let's see what happens if we ignore tokens that appear in less than 30 documents or more than 15% documents. Granted this is arbitrary but a quick search shows tons of methods for reducing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(86 unique tokens: ['two', 'took', 'enjoy', 'wife', 'happi']...)\n",
      "top terms:\n",
      "[('easi', 0), ('took', 1), ('enjoy', 2), ('wife', 3), ('happi', 4), ('one', 5), ('much', 6), ('nice', 7), ('time', 8), ('thank', 9)]\n"
     ]
    }
   ],
   "source": [
    "texts_dict.filter_extremes(no_below=30, no_above=0.15) # inlace filter\n",
    "print(texts_dict)\n",
    "print(\"top terms:\")\n",
    "print(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from **2343** unique tokens to **86** after filtering. Looking at the top 10 tokens it looks like we got more specific subjects opposed to adjectives.\n",
    "\n",
    "### Creating bag of words\n",
    "Next let's turn `texts_dict` into a bag of words instead. doc2bow converts a `document` (a list of words) into the bag-of-words format (list of `(token_id, token_count)` tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [texts_dict.doc2bow(text) for text in texts]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpus is 670 long, the amount of reviews in our dataset and in our dataframe. Let's dump this bag-of-words into a file to avoid parsing the entire text again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Matrix Market format https://radimrehurek.com/gensim/corpora/mmcorpus.html, why exactly? I don't know\n",
    "gensim.corpora.MmCorpus.serialize('amzn_auto_review.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an LDA model\n",
    "Training an LDA model using our BOW corpus as training data requires a number of topics, which is set to 5. The number of passes in the training of the model is set to 100 (should be enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "lda_model = gensim.models.LdaModel(corpus,alpha='auto', num_topics=5,id2word=texts_dict, passes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring Topics \n",
    "Below are the top 5 words associated with 5 random topics. The float next to each word is the weight showing how much the given word influences this specific topic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.119*\"trike\" + 0.043*\"need\" + 0.040*\"one\" + 0.030*\"use\" + 0.027*\"balanc\"'),\n",
       " (1,\n",
       "  '0.063*\"part\" + 0.059*\"work\" + 0.055*\"back\" + 0.038*\"fender\" + 0.036*\"hard\"'),\n",
       " (2,\n",
       "  '0.075*\"wheel\" + 0.071*\"rear\" + 0.061*\"good\" + 0.052*\"fender\" + 0.035*\"one\"'),\n",
       " (3,\n",
       "  '0.050*\"go\" + 0.043*\"seat\" + 0.040*\"basket\" + 0.039*\"tire\" + 0.036*\"got\"'),\n",
       " (4,\n",
       "  '0.095*\"easi\" + 0.055*\"year\" + 0.049*\"bought\" + 0.047*\"well\" + 0.047*\"old\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For `num_topics` number of topics, return `num_words` most significant words\n",
    "lda_model.show_topics(num_topics=5,num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a probabilistic mixture of mixtures (or admixture) model for grouped data. The observed data (words) within the groups (documents) are the result of probabilistically choosing words from a specific topic (multinomial over the vocabulary), where the topic is itself drawn from a document-specific multinomial that has a global Dirichlet prior. This means that words can belong to various topics in various degrees.\n",
    "\n",
    "### Querying the LDA Model\n",
    "Pass an arbitrary string to our model and evaluate what topics are most associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['balanc', 'issu']\n"
     ]
    }
   ],
   "source": [
    "raw_query = 'balance issues'\n",
    "\n",
    "query_words = raw_query.split()\n",
    "query = []\n",
    "for word in query_words:\n",
    "    # ad-hoc reuse steps from above\n",
    "    q_tokens = tokenizer.tokenize(word.lower())\n",
    "    q_stopped_tokens = [word for word in q_tokens if not word in merged_stopwords]\n",
    "    q_stemmed_tokens = [sb_stemmer.stem(word) for word in q_stopped_tokens]\n",
    "    query.append(q_stemmed_tokens[0]) # different frome above, this is not a lists of lists!\n",
    "    \n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# translate words in query to ids and frequencies. \n",
    "id2word = gensim.corpora.Dictionary()\n",
    "_ = id2word.merge_with(texts_dict) # garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31, 1), (49, 1)]\n"
     ]
    }
   ],
   "source": [
    "# translate this document into (word, frequency) pairs\n",
    "query = id2word.doc2bow(query)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this constructed query against our trained mode we will get each topic and the likelihood that the `query` relates to that topic. Remember we arbitrarily specified 5 topics when we made the model. When we organize this list to find the most relative topics, we see some intuitive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.48296359552576401),\n",
       " (1, 0.3931754062744971),\n",
       " (4, 0.045535378867659647),\n",
       " (3, 0.040796348084739094),\n",
       " (2, 0.037529271247339971)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(sorted(lda_model[query], key=lambda x: x[1],reverse=True)) # sort by the second entry in the tuple\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.119*\"trike\" + 0.043*\"need\" + 0.040*\"one\" + 0.030*\"use\" + 0.027*\"balanc\" + 0.026*\"shop\" + 0.025*\"like\" + 0.025*\"year\" + 0.024*\"take\" + 0.023*\"3\"'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topic(a[0][0]) #most related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.075*\"wheel\" + 0.071*\"rear\" + 0.061*\"good\" + 0.052*\"fender\" + 0.035*\"one\" + 0.034*\"product\" + 0.034*\"use\" + 0.028*\"qualiti\" + 0.028*\"order\" + 0.026*\"well\"'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topic(a[-1][0]) #least related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking these inferred topics and analyzing the sentiment of their corresponding documents (reviews) we can find out what customers are saying (or feeling) about specific products. The LDA model can extract representative statements or quotes, enabling us to summarize customersâ€™ opinions about products, perhaps even displaying them on the site. LDA models groups of customers to topics which are groups of products that frequently occur within some customer's orders over time."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
