
% Default to the notebook output style

    


% Inherit from the specified cell style.



    
\documentclass[a4paper,11pt]{article}

    
    
	\usepackage{fancyhdr}
	%\pagestyle{fancy}
	\pagenumbering{roman}
	%\cfoot{Topic Modeling - \thepage}
    \usepackage[T1]{fontenc}
    % Nicer default font than Computer Modern for most use cases
    \usepackage{palatino}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Topic Modeling Amazon Reviews}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{tokenize} \PY{k}{import} \PY{n}{RegexpTokenizer}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{corpus} \PY{k}{import} \PY{n}{stopwords}
        \PY{k+kn}{from} \PY{n+nn}{stop\PYZus{}words} \PY{k}{import} \PY{n}{get\PYZus{}stop\PYZus{}words}
        \PY{k+kn}{from} \PY{n+nn}{nltk}\PY{n+nn}{.}\PY{n+nn}{stem}\PY{n+nn}{.}\PY{n+nn}{snowball} \PY{k}{import} \PY{n}{SnowballStemmer}
        \PY{k+kn}{from} \PY{n+nn}{gensim} \PY{k}{import} \PY{n}{corpora}\PY{p}{,} \PY{n}{models}
        \PY{k+kn}{import} \PY{n+nn}{gensim}
\end{Verbatim}

    \section{Loading our data}\label{loading-our-data}

Loading the provided reviews subset CSV into a Pandas dataframe:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        
        \PY{c+c1}{\PYZsh{}df = pd.read\PYZus{}csv(\PYZsq{}reviews.csv\PYZsq{})}
        
        \PY{n}{df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}table}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reviews\PYZus{}2.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{sep}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{,}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{header}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}
        \PY{n}{df}\PY{o}{.}\PY{n}{columns} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ID}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{numDate}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{prod}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{overall}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{helpful}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{votes}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
        \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{date}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{asin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{summary}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{reviewText}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\end{Verbatim}

    Now that we have a nice corpus of text, let's go through some of the
standard preprocessing required for almost any topic modeling or NLP
problem. This approach will involve:
\begin{enumerate}
\item Tokenizing: converting a document to its atomic elements
\item Stopping: removing meaningless words 
\item Stemming: merging words that are equivalent in meaning
\end{enumerate}


\section{Tokenization}\label{tokenization}

We have many ways to segment our document into its atomic elements. To
start we'll tokenize the document into words. For this instance we'll
use NLTK's \texttt{tokenize.regexp} module. You can see how this works
in a fun interactive way here: try `w+' at http://regexr.com/:
\includegraphics{http://kldavenport.com/wp-content/uploads/2017/03/regex.gif}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{tokenizer} \PY{o}{=} \PY{n}{RegexpTokenizer}\PY{p}{(}\PY{l+s+s1}{r\PYZsq{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{w+}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    Running through part of the first review to demonstrate:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{doc\PYZus{}1} \PY{o}{=} \PY{n}{df}\PY{o}{.}\PY{n}{reviewText}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{c+c1}{\PYZsh{} Using one of our docs as an example}
        \PY{n}{tokens} \PY{o}{=} \PY{n}{tokenizer}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{n}{doc\PYZus{}1}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ characters in string vs }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ words in a list}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{doc\PYZus{}1}\PY{p}{)}\PY{p}{,}                                                             \PY{n+nb}{len}\PY{p}{(}\PY{n}{tokens}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{tokens}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
115 characters in string vs 18 words in a list
['whobbly', 'to', 'ride', 'going', 'around', 'corners', 'very',
'carefully', 'to', 'not']

    \end{Verbatim}

    \section{Stop Words}\label{stop-words}

Determiners like ``the'' and conjunctions such as ``or'' and ``for'' do
not add value to the simple topic model. These types of words are known
as stop words and are desired to be removed them from our list of
tokens. The definition of a stop work changes depending on the context
of the examined documents.

Super list of stop words from the \texttt{stop\_words} and \texttt{nltk}
package below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{merged_stopwords }\OperatorTok{=}\NormalTok{ [}\OperatorTok{*}\NormalTok{nltk_stpwd, }\OperatorTok{*}\NormalTok{stop_words_stpwd]}
\end{Highlighting}
\end{Shaded}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{nltk\PYZus{}stpwd} \PY{o}{=} \PY{n}{stopwords}\PY{o}{.}\PY{n}{words}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{english}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{stop\PYZus{}words\PYZus{}stpwd} \PY{o}{=} \PY{n}{get\PYZus{}stop\PYZus{}words}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{en}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{merged\PYZus{}stopwords} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{nltk\PYZus{}stpwd} \PY{o}{+} \PY{n}{stop\PYZus{}words\PYZus{}stpwd}\PY{p}{)}\PY{p}{)}
        
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n+nb}{set}\PY{p}{(}\PY{n}{merged\PYZus{}stopwords}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{merged\PYZus{}stopwords}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
207
['i',"hadn't","you'd",'theirs','would','between','t','nor','how','more']

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:}
\PY{n}{stopped\PYZus{}tokens}\PY{o}{=}\PY{p}{[}\PY{n}{token} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{tokens} \PY{k}{if} \PY{o+ow}{not} \PY{n}{token} \PY{o+ow}{in} \PY{n}{merged\PYZus{}stopwords}\PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{stopped\PYZus{}tokens}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
['whobbly', 'ride', 'going', 'around', 'corners', 'carefully', 'tip',
'product', 'arrived', 'excellent']

    \end{Verbatim}

    \section{Stemming}\label{stemming}

Stemming allows the reduction of inflectional forms and sometimes
derivationally related forms of a word to a common base form. For
instance, running and runner to run. Another example:

\emph{Amazon's catalog contains bike tires in different sizes and colors
\(\Rightarrow\) Amazon catalog contain bike tire in differ size and
color}

Stemming is a basic and crude heuristic compared to
\href{https://en.wikipedia.org/wiki/Lemmatisation}{Lemmatization} which
understands vocabulary and morphological analysis instead of lobbing off
the end of words. Essentially Lemmatization removes inflectional endings
to return the word to its base or dictionary form of a word, which is
defined as the lemma. Great illustrative examples from Wikipedia:
\begin{enumerate}
\item \emph{The word ``better'' has ``good'' as its lemma. This link is missed
by stemming, as it requires a dictionary look-up.}
\item \emph{The word
``walk'' is the base form for word ``walking'', and hence this is
matched in both stemming and lemmatisation.}
\item \emph{The word
``meeting'' can be either the base form of a noun or a form of a verb
(``to meet'') depending on the context, e.g., ``in our last meeting'' or
``We are meeting again tomorrow''. Unlike stemming, lemmatisation can in
principle select the appropriate lemma depending on the context.}
\end{enumerate}

I start with the common
\href{http://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg}{Snowball
stemming method}, a successor of sorts of the original Porter Stemmer
which is implemented in NLTK:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{} Instantiate a Snowball stemmer}
        \PY{n}{sb\PYZus{}stemmer} \PY{o}{=} \PY{n}{SnowballStemmer}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{english}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}    
\end{Verbatim}

    Note that p\_stemmer requires all tokens to be type str. p\_stemmer
returns the string parameter in stemmed form, so we need to loop through
our stopped\_tokens:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{stemmed\PYZus{}tokens} \PY{o}{=} \PY{p}{[}\PY{n}{sb\PYZus{}stemmer}\PY{o}{.}\PY{n}{stem}\PY{p}{(}\PY{n}{token}\PY{p}{)} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{stopped\PYZus{}tokens}\PY{p}{]}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{stemmed\PYZus{}tokens}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
['whobbl', 'ride', 'go', 'around', 'corner', 'care', 'tip', 'product', 
'arriv', 'excel', 'condit']

    \end{Verbatim}

    \section{Putting together a document-term
matrix}\label{putting-together-a-document-term-matrix}

In order to create an LDA model the 3 steps from above (tokenizing,
stopping, stemming) are needed together to create a list of documents
(list of lists) to then generate a document-term matrix (unique terms as
rows, documents or reviews as columns). This matrix will tell how
frequently each term occurs with each individual document.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:}
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
num\PYZus{}reviews = df.shape[0]
doc\PYZus{}set = [df.reviewText[i] for i in range(num\PYZus{}reviews)]
texts = []

\PY{k}{for} \PY{n}{doc} \PY{o+ow}{in} \PY{n}{doc\PYZus{}set}\PY{p}{:}
	\PYZsh{} putting our three steps together
	\PY{n}{q\PYZus{}tokens} \PY{o}{=} \PY{n}{tokenizer}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{n}{word}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{)}
	\PY{n}{q\PYZus{}stopped\PYZus{}tokens} \PY{o}{=} \PY{p}{[}\PY{n}{token} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{q\PYZus{}tokens} 
			    \PY{k}{if} \PY{o+ow}{not} \PY{n}{token} \PY{o+ow}{in} \PY{n}{merged\PYZus{}stopwords}\PY{p}{]}
	\PY{n}{q\PYZus{}stemmed\PYZus{}tokens}\PY{o}{=}\PY{p}{[}\PY{n}{sb\PYZus{}stemmer}\PY{o}{.}\PY{n}{stem}\PY{p}{(}\PY{n}{token}\PY{p}{)} \PY{k}{for} \PY{n}{token} \PY{o+ow}{in} \PY{n}{q\PYZus{}stopped\PYZus{}tokens}\PY{p}{]}
	\PYZsh{} add tokens to list
	\PY{n}{texts}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{q\PYZus{}stemmed\PYZus{}tokens}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
	
\end{Verbatim}



    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 439 ms

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{n+nb}{print}\PY{p}{(}\PY{n}{texts}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} examine review 1}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
['whobbl', 'ride', 'go', 'around', 'corner', 'care', 'tip',
'product', 'arriv', 'excel', 'condit']

    \end{Verbatim}

    \section{Transform tokenized documents into an id-term
dictionary}\label{transform-tokenized-documents-into-an-id-term-dictionary}

Gensim's Dictionary method encapsulates the mapping between normalized
words and their integer ids. Note a term will have an id of some number
and in the subsequent bag of words step we can see that id will have a
count associated with it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} Gensim\PYZsq{}s Dictionary encapsulates the mapping between 
normalized words and their integer ids.}
         \PY{n}{texts\PYZus{}dict} \PY{o}{=} \PY{n}{corpora}\PY{o}{.}\PY{n}{Dictionary}\PY{p}{(}\PY{n}{texts}\PY{p}{)}
         \PY{n}{texts\PYZus{}dict}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{auto\PYZus{}review.dict}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} lets save to disk for later use}
         \PY{c+c1}{\PYZsh{} Examine each token’s unique id}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{texts\PYZus{}dict}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Dictionary(2343 unique tokens: ['faster', 'thick', 'guard', 'tour', 'teas']{\ldots})

    \end{Verbatim}

    To see the mapping between words and their ids we can use the
\texttt{token2id} method:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kn}{import} \PY{n+nn}{operator}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{IDs 1 through 10: }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{texts\PYZus{}dict}\PY{o}{.}\PY{n}{token2id}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,}
         \PY{n}{key}\PY{o}{=}\PY{n}{operator}\PY{o}{.}\PY{n}{itemgetter}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{n}{reverse} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
IDs 1 through 10: [('tip', 0), ('corner', 1), ('product', 2),
	('go', 3),('ride', 4), ('condit', 5), ('whobbl', 6),
	('around', 7), ('arriv', 8), ('care', 9)]

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{}Guess the original work and examine the count difference between the}
	\PY{c+c1}{\PYZsh{}1 most frequent term and the \PYZsh{}10 most frequent term:}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{reviewText}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{contains}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{balance}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{df}\PY{o}{.}\PY{n}{reviewText}\PY{o}{.}\PY{n}{str}\PY{o}{.}\PY{n}{contains}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{lot}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{value\PYZus{}counts}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
	False    634
	True      36
	Name: reviewText, dtype: int64
	
	False    625
	True      45
	Name: reviewText, dtype: int64

    \end{Verbatim}

    We have a lot of unique tokens, let's see what happens if we ignore
tokens that appear in less than 30 documents or more than 15\%
documents. Granted this is arbitrary but a quick search shows tons of
methods for reducing noise.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{texts\PYZus{}dict}\PY{o}{.}\PY{n}{filter\PYZus{}extremes}\PY{p}{(}\PY{n}{no\PYZus{}below}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{no\PYZus{}above}\PY{o}{=}\PY{l+m+mf}{0.15}\PY{p}{)} \PY{c+c1}{\PYZsh{} inlace filter}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{texts\PYZus{}dict}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{top terms:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{texts\PYZus{}dict}\PY{o}{.}\PY{n}{token2id}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{n}{operator}\PY{o}{.}\PY{n}{itemgetter}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
         \PY{n}{reverse} \PY{o}{=} \PY{k+kc}{False}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{10}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Dictionary(86 unique tokens: ['two', 'took', 'enjoy', 'wife', 'happi']{\ldots})
top terms:
[('easi', 0), ('took', 1), ('enjoy', 2), ('wife', 3), ('happi', 4),
('one', 5),('much', 6), ('nice', 7), ('time', 8), ('thank', 9)]

    \end{Verbatim}

    We went from \textbf{2343} unique tokens to \textbf{86} after filtering.
Looking at the top 10 tokens it looks like we got more specific subjects
opposed to adjectives.

\section{Creating bag of words}\label{creating-bag-of-words}

Next let's turn \texttt{texts\_dict} into a bag of words instead.
doc2bow converts a \texttt{document} (a list of words) into the
bag-of-words format (list of \texttt{(token\_id,\ token\_count)}
tuples).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{n}{corpus} \PY{o}{=} \PY{p}{[}\PY{n}{texts\PYZus{}dict}\PY{o}{.}\PY{n}{doc2bow}\PY{p}{(}\PY{n}{text}\PY{p}{)} \PY{k}{for} \PY{n}{text} \PY{o+ow}{in} \PY{n}{texts}\PY{p}{]}
         \PY{n+nb}{len}\PY{p}{(}\PY{n}{corpus}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}16}]:} 670
\end{Verbatim}
        
    The corpus is 670 long, the amount of reviews in our dataset and in our
dataframe. Let's dump this bag-of-words into a file to avoid parsing the
entire text again:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time} 
         \PYZsh{} Matrix Market format
         https://radimrehurek.com/gensim/corpora/mmcorpus.html
         gensim.corpora.MmCorpus.serialize(\PYZsq{}amzn\PYZus{}auto\PYZus{}review.mm\PYZsq{}, corpus)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 56.5 ms

    \end{Verbatim}

    \section{Training an LDA model}\label{training-an-lda-model}

Training an LDA model using our BOW corpus as training data requires a
number of topics, which is set to 10. The number of passes in the
training of the model is set to 100 (should be enough).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:}
\PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time} 
lda\PYZus{}model = gensim.models.LdaModel(corpus,alpha=\PYZsq{}auto\PYZsq{},
	num\PYZus{}topics=10,id2word=texts\PYZus{}dict, passes=100)
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Wall time: 1min 27s

    \end{Verbatim}

    \section{Inferring Topics}\label{inferring-topics}

Below are the top 5 words associated with 10 random topics. The float
next to each word is the weight showing how much the given word
influences this specific topic.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} For `num\PYZus{}topics` number of topics, }
	\PY{c+c1}{return `num\PYZus{}words` most significant words}
         \PY{n}{lda\PYZus{}model}\PY{o}{.}\PY{n}{show\PYZus{}topics}\PY{p}{(}\PY{n}{num\PYZus{}topics}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{num\PYZus{}words}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}19}]:} [
(0,
'0.138*"year"+0.114*"old"+0.079*"product" + 0.070*"bought" + 0.062*"perfect"'),
(1,
'0.108*"seat"+0.105*"go"+0.075*"got" + 0.062*"basket" + 0.051*"around"'),
(2,
'0.057*"trike"+0.050*"need"+0.048*"fender" + 0.040*"work" + 0.040*"tire"'),
(3,
'0.096*"took"+0.080*"time"+0.065*"new" + 0.057*"littl" + 0.052*"fun"'),
(4,
'0.127*"tricycl"+0.117*"good"+0.104*"qualiti"+0.082*"price"+0.063*"schwinn"'),
(5,
'0.091*"easi"+0.086*"use"+0.058*"realli" + 0.056*"made" + 0.052*"take"'),
(6,
'0.118*"fender"+0.107*"rear"+0.069*"box" + 0.049*"part" + 0.047*"wheel"'),
(7,
'0.100*"nice"+0.048*"want"+0.046*"happi" + 0.043*"one" + 0.043*"sturdi"'),
(8,
'0.152*"wheel"+0.057*"trike"+0.051*"3" + 0.047*"brake" + 0.042*"difficult"'),
(9,
'0.142*"make"+0.128*"thank"+0.080*"easi" + 0.077*"shop" + 0.059*"got"')]
\end{Verbatim}
        
    LDA is a probabilistic mixture of mixtures (or admixture) model for
grouped data. The observed data (words) within the groups (documents)
are the result of probabilistically choosing words from a specific topic
(multinomial over the vocabulary), where the topic is itself drawn from
a document-specific multinomial that has a global Dirichlet prior. This
means that words can belong to various topics in various degrees.

\section{Querying the LDA Model}\label{querying-the-lda-model}

Pass an arbitrary string to our model and evaluate what topics are most
associated with it.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} 
\PY{n}{raw\PYZus{}query} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{issue}\PY{l+s+s1}{\PYZsq{}}
         
\PY{n}{query\PYZus{}words} \PY{o}{=} \PY{n}{raw\PYZus{}query}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
\PY{n}{query} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{query\PYZus{}words}\PY{p}{:}
	\PY{c+c1}{\PYZsh{} ad\PYZhy{}hoc reuse steps from above}
	\PY{n}{q\PYZus{}tokens} \PY{o}{=} \PY{n}{tokenizer}\PY{o}{.}\PY{n}{tokenize}\PY{p}{(}\PY{n}{word}\PY{o}{.}\PY{n}{lower}\PY{p}{(}\PY{p}{)}\PY{p}{)}
	\PY{n}{q\PYZus{}stopped\PYZus{}tokens} \PY{o}{=} \PY{p}{[}\PY{n}{word} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{q\PYZus{}tokens} \PY{k}{if} \PY{o+ow}{not} \PY{n}{word} \PY{o+ow}{in} \PY{n}{merged\PYZus{}stopwords}\PY{p}{]}
	\PY{n}{q\PYZus{}stemmed\PYZus{}tokens} \PY{o}{=} \PY{p}{[}\PY{n}{sb\PYZus{}stemmer}\PY{o}{.}\PY{n}{stem}\PY{p}{(}\PY{n}{word}\PY{p}{)} \PY{k}{for} \PY{n}{word} \PY{o+ow}{in} \PY{n}{q\PYZus{}stopped\PYZus{}tokens}\PY{p}{]}
	\PY{n}{query}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{q\PYZus{}stemmed\PYZus{}tokens}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
	\PY{c+c1}{\PYZsh{}different from above, this is not a lists of lists!}
             
\PY{n+nb}{print}\PY{p}{(}\PY{n}{query}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
['issu']

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} translate words in query to ids and frequencies. }
         \PY{n}{id2word} \PY{o}{=} \PY{n}{gensim}\PY{o}{.}\PY{n}{corpora}\PY{o}{.}\PY{n}{Dictionary}\PY{p}{(}\PY{p}{)}
         \PY{n}{\PYZus{}} \PY{o}{=} \PY{n}{id2word}\PY{o}{.}\PY{n}{merge\PYZus{}with}\PY{p}{(}\PY{n}{texts\PYZus{}dict}\PY{p}{)} \PY{c+c1}{\PYZsh{} garbage}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{} translate this document into (word, frequency) pairs}
         \PY{n}{query} \PY{o}{=} \PY{n}{id2word}\PY{o}{.}\PY{n}{doc2bow}\PY{p}{(}\PY{n}{query}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{query}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[(18, 1)]

    \end{Verbatim}

    If we run this constructed query against our trained mode we will get
each topic and the likelihood that the \texttt{query} relates to that
topic. Remember we arbitrarily specified 10 topics when we made the
model. When we organize this list to find the most relative topics, we
see some intuitive results.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{n}{a} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{sorted}\PY{p}{(}\PY{n}{lda\PYZus{}model}\PY{p}{[}\PY{n}{query}\PY{p}{]}\PY{p}{,} \PY{n}{key}\PY{o}{=}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{reverse}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
\PY{c+c1}{\PYZsh{} sort by the second entry in the tuple}
         \PY{n}{a}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}23}]:} [(3, 0.66089604585589257),
 (0, 0.042840632894293761),
 (7, 0.041895896144469633),
 (2, 0.039156986576921418),
 (4, 0.038719777833920874),
 (5, 0.038465777946288152),
 (1, 0.038185859271443023),
 (8, 0.035220474260673415),
 (6, 0.03455538873028282),
 (9, 0.030063160485814349)]
\end{Verbatim}
        
        \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{lda\PYZus{}model}\PY{o}{.}\PY{n}{print\PYZus{}topic}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}most related}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:} '0.096*"took" + 0.080*"time" + 0.065*"new" + 0.057*"littl" + 
	0.052*"fun" + 0.048*"hour" + 0.048*"husband" + 0.042*"two" +
	0.042*"like" + 0.042*"wife"'
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{lda\PYZus{}model}\PY{o}{.}\PY{n}{print\PYZus{}topic}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{}least related}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}25}]:} '0.142*"make" + 0.128*"thank" + 0.080*"easi" + 0.077*"shop" +
	0.059*"got" + 0.045*"take" + 0.043*"back" + 0.040*"pedal" +
	0.033*"handl" + 0.029*"littl"'
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
         \PY{n}{word\PYZus{}least} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{lda\PYZus{}model}\PY{o}{.}\PY{n}{show\PYZus{}topic}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}
         \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
         \PY{n}{word\PYZus{}most} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{lda\PYZus{}model}\PY{o}{.}\PY{n}{show\PYZus{}topic}\PY{p}{(}\PY{n}{a}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}
         \PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
         \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
         \PY{n}{word\PYZus{}least}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}EE4266}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         	\PY{n}{x}\PY{o}{=}\PY{n}{word\PYZus{}least}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
         	\PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Words of the Less Related Topic about }\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{Issues}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
		\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
         \PY{n}{word\PYZus{}most}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsh{}5DFDCB}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         	\PY{n}{x}\PY{o}{=}\PY{n}{word\PYZus{}most}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{words}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,}\PY{n}{legend}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,}
         	\PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Words of the Most Related Topic about }\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{Issues}\PY{l+s+s1}{\PYZdq{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}
         	\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}27}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x2c6c39e7978>
\end{Verbatim}
\section{Conclusion}\label{conclusion}
	
	Our query "issues" seems to be highly related to the topic number 3, with a correlation of 66\%. The rest of topic are mostly unrelated, all below 5\% correlation.
   
    By taking these inferred topics and analyzing the sentiment of their
corresponding documents (reviews) we can find out what customers are
saying (or feeling) about specific products. The LDA model can extract
representative statements or quotes, enabling us to summarize customers'
opinions about products, perhaps even displaying them on the site. LDA
models groups of customers to topics which are groups of products that
frequently occur within some customer's orders over time.
        
    \begin{center}
    \adjustimage{max size={0.8\linewidth}{0.8\paperheight}}{Topic_Modeling_Amazon_Reviews_files/Topic_Modeling_Amazon_Reviews2_42_1.png}
    \end{center}
    
    \begin{center}
    \adjustimage{max size={0.8\linewidth}{0.8\paperheight}}{Topic_Modeling_Amazon_Reviews_files/Topic_Modeling_Amazon_Reviews2_42_2.png}
    \end{center}

    


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
